{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT MODULES\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SELECT EXPERIMENT TYPE\n",
    "\n",
    "# p= 0.5\n",
    "# experiment = 1.1 ## bernoulli p=0.5 [0,1]\n",
    "\n",
    "# p = 0.8\n",
    "# experiment = 1.2 # bernoulli p=0.8 [0,1] [0.2,0.8]\n",
    "\n",
    "# p = 0.2\n",
    "# experiment = 1.3 # bernoulli p=0.2 [0,1] [0.8,0.2]\n",
    "\n",
    "# lam = 0.5\n",
    "# experiment = 2 ## poisson, lam = 0.5 (avg arrivals per unit time)\n",
    "\n",
    "lam = 0.01\n",
    "experiment = 2.2 ## poisson, lam = 0.01 (avg arrivals per unit time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXPERIMENT PARAMETERS\n",
    "\n",
    "# Define the parameters\n",
    "N = 20  # Maximum queue length\n",
    "M = 18   # Maximum multiplier for random value\n",
    "\n",
    "states_x = list(range(N + 1))\n",
    "states_y = [0, 1]\n",
    "actions_u = list(range(2))  # Actions u = {0, 1}\n",
    "actions_v = [0, 1]  # Actions v = {0, 1}\n",
    "\n",
    "#arrival_probabilities = [1-p,p]\n",
    "\n",
    "# Define the probabilities for y\n",
    "\n",
    "eta = 0.3\n",
    "prob_y = {\n",
    "    (0, 0): 1-eta,\n",
    "    (0, 1): eta,\n",
    "    (1, 0): eta,\n",
    "    (1, 1): 1-eta\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRANSITION PROBABILITIES EXCLUDING CHANNEL STATE\n",
    "\n",
    "def simulate(x, u, v):\n",
    "    \"\"\"Simulate the next state and return the probabilities.\"\"\"\n",
    "    transitions = defaultdict(float)\n",
    "    \n",
    "    # Calculate all possible transitions for the queue length x\n",
    "    for k in range(2):\n",
    "        z = v * k\n",
    "        x_next = min(N, x - min(x, u) + z)\n",
    "        transitions[x_next] += arrival_probabilities[k]\n",
    "    \n",
    "    return transitions\n",
    "\n",
    "def calculate_transition_probabilities():\n",
    "    \"\"\"Calculate transition probabilities P(s' | s, a) for all (s, a) pairs.\"\"\"\n",
    "    transition_probs = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    # Iterate over all possible states and actions\n",
    "    for x in states_x:\n",
    "        for u in actions_u:\n",
    "            for v in actions_v:\n",
    "                # Simulate the next state probabilities\n",
    "                transitions = simulate(x,u,v)\n",
    "                for next_x, prob in transitions.items():\n",
    "                    transition_probs[(x,u,v)][next_x] += round(prob,2)\n",
    "\n",
    "    return transition_probs\n",
    "\n",
    "# Calculate the transition probabilities for all state-action pairs\n",
    "transition_probs = calculate_transition_probabilities()\n",
    "\n",
    "\n",
    "def plot_transition_probabilities(transition_probs, states_x, actions_u, actions_v):\n",
    "    \"\"\"Plot the transition probabilities.\"\"\"\n",
    "    fig, axes = plt.subplots(len(actions_u), len(actions_v), figsize=(15, 10), sharex=True, sharey=True)\n",
    "    if len(actions_u) == 1 and len(actions_v) == 1:\n",
    "        axes = [[axes]]\n",
    "    elif len(actions_u) == 1 or len(actions_v) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, u in enumerate(actions_u):\n",
    "        for j, v in enumerate(actions_v):\n",
    "            # Create a matrix of transition probabilities\n",
    "            prob_matrix = np.zeros((len(states_x), len(states_x)))\n",
    "            for x in states_x:\n",
    "                for next_x, prob in transition_probs[(x, u, v)].items():\n",
    "                    prob_matrix[x, next_x] = prob\n",
    "\n",
    "            ax = axes[i][j]\n",
    "            sns.heatmap(prob_matrix, annot=False, fmt=\".2f\", cmap=\"viridis\", ax=ax)\n",
    "                     \n",
    "            # Annotate only the center and corner squares\n",
    "            for x in [0, len(states_x) - 1, len(states_x) // 2]:\n",
    "                for y in [0, len(states_x) - 1, len(states_x) // 2]:\n",
    "                    if x < len(states_x) and y < len(states_x):  # Ensure indexes are within bounds\n",
    "                        ax.text(y+0.5, x+0.5, f\"{prob_matrix[x, y]:.1f}\", \n",
    "                                ha='center', va='center', color='black')\n",
    "           \n",
    "            ax.set_title(f\"u={u}, v={v}\")\n",
    "            ax.set_xlabel(\"Next State\")\n",
    "            ax.set_ylabel(\"Current State\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Experiment{experiment}_transitions.png\",dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the transition probabilities\n",
    "plot_transition_probabilities(transition_probs, states_x, actions_u, actions_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRANSITION PROBABILITIES INCLUDING CHANNEL STATE\n",
    "\n",
    "\n",
    "def simulate(x, y, u, v):\n",
    "    \"\"\"Simulate the next state and return the probabilities.\"\"\"\n",
    "    transitions = defaultdict(float)\n",
    "    \n",
    "    # Calculate all possible transitions for the queue length x\n",
    "    for a, p_a in zip([0, 1], arrival_probabilities):\n",
    "        z = a * v\n",
    "        x_next = min(N, x - min(x, u) + z)\n",
    "        transitions[(x_next, 0)] += prob_y[(y, 0)] * p_a\n",
    "        transitions[(x_next, 1)] += prob_y[(y, 1)] * p_a\n",
    "    \n",
    "    return transitions\n",
    "\n",
    "def calculate_transition_probabilities():\n",
    "    \"\"\"Calculate transition probabilities P(s' | s, a) for all (s, a) pairs.\"\"\"\n",
    "    transition_probs = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    # Iterate over all possible states and actions\n",
    "    for x in states_x:\n",
    "        for y in states_y:\n",
    "            for u in actions_u:\n",
    "                for v in actions_v:\n",
    "                    # Simulate the next state probabilities\n",
    "                    transitions = simulate(x, y, u, v)\n",
    "                    for (next_x, next_y), prob in transitions.items():\n",
    "                        transition_probs[(x, y, u, v)][(next_x, next_y)] += prob\n",
    "    \n",
    "    return transition_probs\n",
    "\n",
    "\n",
    "# Calculate the transition probabilities\n",
    "transition_probs = calculate_transition_probabilities()\n",
    "\n",
    "# Print the transition probabilities for verification\n",
    "for state_action, transitions in transition_probs.items():\n",
    "    print(f\"From state-action {state_action}:\")\n",
    "    for next_state, prob in transitions.items():\n",
    "        print(f\"  To next state {next_state} with probability {prob:.4f}\")\n",
    "\n",
    "# Convert transition_probs dictionary to a list of tuples\n",
    "transition_probs_list = []\n",
    "\n",
    "for state_action, transitions in transition_probs.items():\n",
    "    for next_state, prob in transitions.items():\n",
    "        transition_probs_list.append((*state_action, *next_state, prob))\n",
    "\n",
    "\n",
    "# Function to plot heatmaps\n",
    "def plot_heatmap(probabilities, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(probabilities, cmap='viridis', interpolation='nearest', vmin=0, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Next State (x)')\n",
    "    plt.ylabel('Current State (x)')\n",
    "    plt.title(title)\n",
    "    plt.xticks(np.arange(len(states_x)), states_x)\n",
    "    plt.yticks(np.arange(len(states_x)), states_x)\n",
    "    \n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to match matrix representation\n",
    "    # Add annotations for probability values in specific positions\n",
    "    for i, j in [(0, 0), (0, len(states_x)//2), (0, len(states_x)-1),\n",
    "                 (len(states_x)//2, 0), (len(states_x)//2, len(states_x)//2),\n",
    "                 (len(states_x)//2, len(states_x)-1),\n",
    "                 (len(states_x)-1, 0), (len(states_x)-1, len(states_x)//2),\n",
    "                 (len(states_x)-1, len(states_x)-1)]:\n",
    "        text = f'{probabilities[i, j]:.2f}'  # Format probability to 2 decimal places\n",
    "        plt.text(j, i, text, ha='center', va='center', color='white')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Iterate over each (y, u, v) tuple and plot heatmap\n",
    "\n",
    "for nexty in [0]:\n",
    "    for y in [0]:\n",
    "        for u in actions_u:\n",
    "            for v in actions_v:\n",
    "                # Initialize a matrix to store probabilities\n",
    "                probabilities_matrix = np.zeros((len(states_x), len(states_x)))\n",
    "\n",
    "                # Find the relevant entries in transition_probs_list for (y, u, v)\n",
    "                relevant_entries = [(x, next_x, prob) for x, y_, u_, v_, next_x, nexty_, prob \n",
    "                                    in transition_probs_list if y == y_ and u == u_ and v == v_ and nexty == nexty_]\n",
    "\n",
    "                # Populate the matrix with transition probabilities\n",
    "                for x, next_x, prob in relevant_entries:\n",
    "                    probabilities_matrix[x, next_x] = prob\n",
    "\n",
    "                # Plot the heatmap\n",
    "                title = f\"Heatmap for (y={y}, y_next = {nexty}, u={u}, v={v})\"\n",
    "                plot_heatmap(probabilities_matrix, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `states_x`, `actions_u`, `actions_v`, and `transition_probs_list` are already defined.\n",
    "\n",
    "# Function to plot heatmaps\n",
    "def plot_heatmap(ax, probabilities, subtitle):\n",
    "    cax = ax.imshow(probabilities, cmap='viridis', interpolation='nearest', vmin=0, vmax=1)\n",
    "    ax.set_xlabel('Next State (x)')\n",
    "    ax.set_ylabel('Current State (x)')\n",
    "    ax.set_title(subtitle, fontsize=10)\n",
    "    ax.set_xticks(np.arange(len(states_x)))\n",
    "    ax.set_yticks(np.arange(len(states_x)))\n",
    "    ax.set_xticklabels(states_x)\n",
    "    ax.set_yticklabels(states_x)\n",
    "\n",
    "    # Add annotations for probability values in specific positions\n",
    "    for i, j in [(0, 0), (0, len(states_x)//2), (0, len(states_x)-1),\n",
    "                 (len(states_x)//2, 0), (len(states_x)//2, len(states_x)//2),\n",
    "                 (len(states_x)//2, len(states_x)-1),\n",
    "                 (len(states_x)-1, 0), (len(states_x)-1, len(states_x)//2),\n",
    "                 (len(states_x)-1, len(states_x)-1)]:\n",
    "        text = f'{probabilities[i, j]:.2f}'  # Format probability to 2 decimal places\n",
    "        ax.text(j, i, text, ha='center', va='center', color='white')\n",
    "    return cax\n",
    "\n",
    "# Number of (y, nexty) pairs\n",
    "num_rows = 4\n",
    "\n",
    "# Number of (u, v) pairs\n",
    "num_cols = len(actions_u) * len(actions_v)\n",
    "\n",
    "# Create a larger grid of subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(num_cols * 4, num_rows * 4))\n",
    "axs = axs.reshape(num_rows, num_cols)  # Reshape axs to a 2D array for easy indexing\n",
    "\n",
    "# Initialize a counter for subplot index\n",
    "subplot_idx = 0\n",
    "\n",
    "# Iterate over each (nexty, y) pair and plot heatmaps\n",
    "row_idx = 0\n",
    "for nexty in [0, 1]:\n",
    "    for y in [0, 1]:\n",
    "        col_idx = 0\n",
    "        for u in actions_u:\n",
    "            for v in actions_v:\n",
    "                # Initialize a matrix to store probabilities\n",
    "                probabilities_matrix = np.zeros((len(states_x), len(states_x)))\n",
    "\n",
    "                # Find the relevant entries in transition_probs_list for (y, u, v)\n",
    "                relevant_entries = [(x, next_x, prob) for x, y_, u_, v_, next_x, nexty_, prob \n",
    "                                    in transition_probs_list if y == y_ and u == u_ and v == v_ and nexty == nexty_]\n",
    "\n",
    "                # Populate the matrix with transition probabilities\n",
    "                for x, next_x, prob in relevant_entries:\n",
    "                    probabilities_matrix[x, next_x] = prob\n",
    "\n",
    "                # Plot the heatmap on the current subplot\n",
    "                subtitle = f\"u={u}, v={v}\"\n",
    "                cax = plot_heatmap(axs[row_idx, col_idx], probabilities_matrix, subtitle)\n",
    "                col_idx += 1\n",
    "\n",
    "        # Add a title above the current row of subplots\n",
    "        fig.text(0.5, 1 - (row_idx+1)*(1)/ num_rows + 0.02, f'Heatmaps for y={y}, y_next={nexty}', ha='center', va='bottom', fontsize=16, transform=fig.transFigure, color='orange')\n",
    "        row_idx += 1\n",
    "\n",
    "# Adjust layout to make space for the titles and colorbar\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# Add a colorbar to the side\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # Position: [left, bottom, width, height]\n",
    "fig.colorbar(cax, cax=cbar_ax)\n",
    "plt.savefig(f'Experiment{experiment}_transitions_withY.png',dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COST FUNCTIONS\n",
    "\n",
    "def cost(q):\n",
    "    return np.exp((np.maximum(q-14, 0)))\n",
    "\n",
    "# Generate q values over the entire range\n",
    "marker_q_values = np.arange(N+1)\n",
    "q_values = np.array(marker_q_values)\n",
    "\n",
    "# Plot the reward function with markers\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the regular function\n",
    "plt.plot(q_values, cost(q_values), label='Cost Function')\n",
    "\n",
    "# Add markers at specific values of q\n",
    "\n",
    "plt.scatter(marker_q_values, cost(np.array(marker_q_values)), color='red')\n",
    "\n",
    "# Labeling and grid\n",
    "plt.title(f'Cost Function for N={N} and M={M}')\n",
    "plt.xlabel('q')\n",
    "plt.xticks(np.arange(N+1))\n",
    "plt.ylabel(r'Cost = $e^{(q-14)^{+}}$', fontsize=12) \n",
    "plt.grid(True)\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "plt.savefig(f'Experiment{experiment}_cost.png',dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "eta = 0.3\n",
    "# Define Markov chain transition probabilities for the slower Q-learning\n",
    "transition_matrix = {0: {0: 1 - eta, 1: eta}, 1: {0: eta, 1: 1 - eta}}\n",
    "\n",
    "# Function to update Markov chain state for the slower Q-learning\n",
    "def update_markov_chain_state(current_state):\n",
    "    next_state = np.random.choice(list(transition_matrix[current_state].keys()), \n",
    "                                p=list(transition_matrix[current_state].values()))\n",
    "    return next_state\n",
    "\n",
    "\n",
    "########### COST FUNCTION for Slow Q Learning:\n",
    "\n",
    "# Function to calculate the cost for the slower Q-learning\n",
    "def k(i, u, y):\n",
    "    if y == 0:\n",
    "        a = 1\n",
    "    elif y == 1:\n",
    "        a = 6\n",
    "    # Adjusted value for C\n",
    "\n",
    "    term1 = ((a * min(i, u)) ** 2) \n",
    "    term2 = 2*(i)\n",
    "    return term1+term2\n",
    "\n",
    "i_values = np.arange(0, N+1)\n",
    "u_values = np.arange(2)\n",
    "\n",
    "# Calculate costs to determine the color scale limits\n",
    "costs_all = []\n",
    "for y in [0, 1]:\n",
    "    costs = np.zeros((N+1, 2))\n",
    "    for i in i_values:\n",
    "        for u in u_values:\n",
    "            costs[i, u] = k(i, u, y)\n",
    "    costs_all.append(costs)\n",
    "\n",
    "# Determine the global min and max costs for color scaling\n",
    "global_min = np.min(costs_all)\n",
    "global_max = np.max(costs_all)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "for y in [0, 1]:\n",
    "    costs = costs_all[y]\n",
    "    state_label = 'good' if y == 0 else 'bad'\n",
    "    \n",
    "    ax = axs[y]\n",
    "    c = ax.pcolormesh(u_values, i_values, costs, shading='auto', cmap='viridis', vmin=global_min, vmax=global_max)\n",
    "    ax.set_title(f'Cost function for y={y} ({state_label} state)', fontsize=20)\n",
    "    ax.set_xlabel('Departure Action u', fontsize=20)\n",
    "    ax.set_ylabel('Queue length i', fontsize=20)\n",
    "    ax.set_xticks(u_values)\n",
    "    ax.set_yticks(i_values)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    fig.colorbar(c, ax=ax)\n",
    "\n",
    "# Set the same y-axis limits and y-ticks for both plots\n",
    "for ax in axs:\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, N)\n",
    "    ax.set_yticks(np.arange(0, N+1, 5))  # Set y-ticks at intervals of 5 for clarity\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'RiskClassic_classicalcost.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MIN MIN FUNCTION\n",
    "\n",
    "\n",
    "def minmin(Q, X):\n",
    "    # Extract the slice of Q for the given X\n",
    "    Q_slice = Q[X, :, :]\n",
    "    \n",
    "    # Find the minimum value\n",
    "    min_value = np.min(Q_slice)\n",
    "    \n",
    "    # Find indices of the minimum value\n",
    "    min_indices = np.where(Q_slice == min_value)\n",
    "    \n",
    "    # Prepare a list of indices\n",
    "    min_indices_list = list(zip(min_indices[0], min_indices[1]))\n",
    "    \n",
    "    return min_value, min_indices_list\n",
    "\n",
    "def minmin2(Q, X,Y):\n",
    "    # Extract the slice of Q for the given X\n",
    "    Q_slice = Q[X,Y, :, :]\n",
    "    \n",
    "    # Find the minimum value\n",
    "    min_value = np.min(Q_slice)\n",
    "    \n",
    "    # Find indices of the minimum value\n",
    "    min_indices = np.where(Q_slice == min_value)\n",
    "    \n",
    "    # Prepare a list of indices\n",
    "    min_indices_list = list(zip(min_indices[0], min_indices[1]))\n",
    "    \n",
    "    return min_value, min_indices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Q VALUE ITERATION ############\n",
    "\n",
    "delta_store = []\n",
    "time_store = []\n",
    "\n",
    "def q_value_iteration(states_x, actions_v, transition_probs, gamma=1, theta=1e-6):\n",
    "    \"\"\"Perform Q-value iteration to find the optimal action-value function.\"\"\"\n",
    "    # Initialize the Q-values arbitrarily\n",
    "\n",
    "    #Q = np.multiply(100,np.random.rand(N + 1,2, M + 1,2))\n",
    "    Q = np.multiply(np.random.rand(N + 1,2,2),100)\n",
    "\n",
    "    x = 0\n",
    "    u = 0\n",
    "    v = 0\n",
    "    \n",
    "    # Iterate until convergence\n",
    "    \n",
    "    step=0\n",
    "    while True:\n",
    "        delta = 0\n",
    "\n",
    "        for x in states_x:\n",
    "            #for y in states_y:\n",
    "               # for u in actions_u:\n",
    "            for u in actions_u:\n",
    "                for v in actions_v:\n",
    "\n",
    "                    old_q_value = Q[x,u,v]\n",
    "                    new_q_value = 0\n",
    "\n",
    "                    # Calculate the new Q-value using the Bellman equation\n",
    "\n",
    "                    normalization_factor = 1\n",
    "                    sum_prob_S0 = sum(transition_probs[x,u,v][j] for j in np.arange(M+1) if j in transition_probs[x,u,v])\n",
    "                    cost_term = cost(x)\n",
    "\n",
    "                    for next_x, prob in transition_probs[(x, u,v)].items():\n",
    "\n",
    "                        #miniminQ, _, _ = minmin(Q,next_x,next_y)\n",
    "                        miniminQ = np.min(Q[next_x,:])\n",
    "                        #new_q_value += prob *  ((identity * cost(x) * miniminQ)/Q[0,0]) \n",
    "                        new_q_value += prob * miniminQ\n",
    "\n",
    "                    # if x==18 and u==0:\n",
    "                    #     #print(\"step\", step, \"v\",v, \"min_sum\", new_q_value, \"s0_sum\", sum_prob_S0)\n",
    "                    #     print(\"___\")\n",
    "                    new_q_value = new_q_value * cost_term * sum_prob_S0/normalization_factor\n",
    "\n",
    "                \n",
    "                    \n",
    "                    # Update the Q-value\n",
    "            \n",
    "                    Q[x,u,v] = new_q_value\n",
    "                \n",
    "                    #print(new_q_value-q)\n",
    "                    #print(x,y,u,v,Q[x,y,u,v])\n",
    "                    delta = max(delta,old_q_value-new_q_value)\n",
    "                    \n",
    "        \n",
    "                     \n",
    "        \n",
    "        #if step%10==0:\n",
    "        print(\"step\", step, \"del\",delta)\n",
    "        delta_store.append(delta)\n",
    "        time_store.append(step)\n",
    "\n",
    "    \n",
    "        step+=1\n",
    "        # Check for convergence\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "\n",
    "    return Q\n",
    "\n",
    "optimal_q_values = q_value_iteration(states_x, actions_v, transition_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### VALUE ITERATION CONVERGENCE\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Plot the first dataset\n",
    "axs[0].plot(time_store, delta_store)\n",
    "#axs[0].set_title('Delta (Max Change in Q-Value per Iteration) over Time')\n",
    "axs[0].set_xlabel('Time Steps', fontsize=14)\n",
    "axs[0].set_ylabel('Delta', fontsize=14)\n",
    "axs[0].set_xticks(range(0, len(delta_store) + 1, 100))\n",
    "axs[0].grid(False)\n",
    "\n",
    "# Plot the second dataset\n",
    "axs[1].plot(time_store[10:], delta_store[10:])\n",
    "axs[1].set_xlabel('Time Steps', fontsize=14)\n",
    "axs[1].set_ylabel('Delta', fontsize=14)\n",
    "axs[1].set_xticks(range(10, len(delta_store) + 1, 100))\n",
    "axs[1].grid(False)\n",
    "\n",
    "# Adjust layout to prevent overlapping titles\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(f'Experiment{experiment}_delta_subplots.png', dpi=300)\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LEARNING RATES #########\n",
    "\n",
    "# Define learning rates as functions of 'a(n)' and 'b(n)'\n",
    "c1 = 0.001\n",
    "\n",
    "def a_learning_rate(n):\n",
    "    return c1 / (1 + math.ceil(n /125))\n",
    "\n",
    "# Define learning rates as functions of 'a(n)' and 'b(n)'\n",
    "def b_learning_rate(n):\n",
    "    return c1 / (1 + math.ceil(((n/25+1)*np.log(n/25+1)) /250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GENERATING POISSON ARRIVALS\n",
    "\n",
    "def generate_poisson_arrivals(lam, duration_steps):\n",
    "    arrivals = np.zeros(duration_steps, dtype=int)\n",
    "    time = 0\n",
    "    while time < duration_steps:\n",
    "        interval = np.random.exponential(scale=1/lam)\n",
    "        time += interval\n",
    "        if int(time) < duration_steps:\n",
    "            arrivals[int(time)] = 1\n",
    "    return arrivals\n",
    "\n",
    " # Rate of lam events per time step\n",
    "duration_steps = 1000000 # Total duration of 100 time steps\n",
    "\n",
    "arrivals = generate_poisson_arrivals(lam, duration_steps)\n",
    "print(arrivals[1000:1100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aggregate arrivals into bins\n",
    "bin_size = 10000//2\n",
    "# Aggregate arrivals into bins\n",
    "num_bins = duration_steps // bin_size\n",
    "binned_arrivals = np.add.reduceat(arrivals, np.arange(0, duration_steps, bin_size))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(np.arange(num_bins) * bin_size, binned_arrivals, width=bin_size, color='b', alpha=0.7)\n",
    "\n",
    "# Annotate plot\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Number of Arrivals')\n",
    "plt.title(f'Poisson Arrival Distribution (Rate λ={lam})')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Annotate the bin size\n",
    "plt.text(0.02, 0.95, 'Bin size: {}'.format(bin_size), transform=plt.gca().transAxes,\n",
    "         fontsize=12, verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor='black', facecolor='white'))\n",
    "\n",
    "# Annotate the max number of arrivals in any bin\n",
    "max_arrivals = max(binned_arrivals)\n",
    "max_bin = np.argmax(binned_arrivals) * bin_size\n",
    "plt.annotate('Max Arrivals: {}'.format(max_arrivals),\n",
    "             xy=(max_bin, max_arrivals), xytext=(max_bin + duration_steps * 0.05, max_arrivals * 1.1),\n",
    "             arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "             fontsize=12, color='red')\n",
    "\n",
    "plt.savefig(f'Experiment{experiment}_arrivalsim.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "def generate_inter_arrival_times(lam, duration_steps):\n",
    "    inter_arrival_times = []\n",
    "    time = 0\n",
    "    while time < duration_steps:\n",
    "        interval = np.random.exponential(scale=1/lam)\n",
    "        inter_arrival_times.append(interval)\n",
    "        time += interval\n",
    "    return inter_arrival_times\n",
    "\n",
    "# Parameters\n",
    "lam = 0.5  # Poisson rate\n",
    "duration_steps = 1000000  # Total time steps\n",
    "\n",
    "# Generate inter-arrival times\n",
    "inter_arrival_times = generate_inter_arrival_times(lam, duration_steps)\n",
    "\n",
    "# Theoretical exponential distribution\n",
    "x_values = np.linspace(0, max(inter_arrival_times), 1000)\n",
    "pdf_values = expon.pdf(x_values, scale=1/lam)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(inter_arrival_times, bins=100, density=True, alpha=0.7, color='b', label='Simulated Inter-Arrival Times')\n",
    "plt.plot(x_values, pdf_values, 'r-', label='Theoretical Exponential Distribution')\n",
    "plt.xlabel('Time Between Arrivals')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Inter-Arrival Time Distribution (Rate λ={})'.format(lam))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'Experiment{experiment}_arrivaltime.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Q LEARNING ##############\n",
    "\n",
    "delta_store = []\n",
    "time_store = []\n",
    "changer = [[[[None for _ in range(0)] for _ in range(2)] for _ in range(2)] for _ in range(N + 1)]\n",
    "def q_learning(states_x, actions_v, transition_probs, gamma=1, theta=1e-5):\n",
    "    \"\"\"Perform Q-value iteration to find the optimal action-value function.\"\"\"\n",
    "    # Initialize the Q-values arbitrarily\n",
    "\n",
    "    Q_fast = np.multiply(np.random.rand(N + 1,2,2)+1,100)\n",
    "\n",
    "    # Iterate until convergence\n",
    "    \n",
    "    step=0\n",
    "    while True:\n",
    "        delta = 0\n",
    "\n",
    "        alpha_slow = a_learning_rate(step)\n",
    "\n",
    "        for x in states_x:\n",
    "            #for y in states_y:\n",
    "               # for u in actions_u:\n",
    "            for u in actions_u:\n",
    "                for v in actions_v:\n",
    "\n",
    "\n",
    "\n",
    "                    old_q_value = Q_fast[x,u,v]\n",
    "                    new_q_value = 0\n",
    "                    arrival = arrivals[step]\n",
    "                    x_next = min(N, x - min(u,x) + v*arrival)\n",
    "                    normalization_factor = 1\n",
    "                    cost_term = cost(x)\n",
    "                    if x_next < M+1:\n",
    "                        identity = 1\n",
    "                        miniminQ, _ = minmin(Q_fast, x_next)\n",
    "\n",
    "                        change = alpha_slow * (identity * cost_term * miniminQ)/normalization_factor\n",
    "                    else:\n",
    "                        identity=0\n",
    "                        change = 0\n",
    "                    new_q_value = (1-alpha_slow)*old_q_value + alpha_slow*(change)\n",
    "\n",
    "                    changer[x][u][v].append(abs(alpha_slow*(change - old_q_value)))\n",
    "            \n",
    "                    Q_fast[x,u,v] = new_q_value\n",
    "\n",
    "\n",
    "              \n",
    "\n",
    "\n",
    "\n",
    "                    delta = max(delta,old_q_value-new_q_value)\n",
    "                    \n",
    "        \n",
    "                     \n",
    "        \n",
    "        #if step%10==0:\n",
    "        print(\"step\", step, \"del\",delta)\n",
    "        delta_store.append(delta)\n",
    "        time_store.append(step)\n",
    "\n",
    "    \n",
    "        step+=1\n",
    "        # Check for convergence\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "\n",
    "    return Q_fast\n",
    "\n",
    "optimal_q_learning  = q_learning(states_x, actions_v, transition_probs)\n",
    "print(optimal_q_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(21):\n",
    "    for u in range(2):\n",
    "        for v in range(2):\n",
    "            print(f\"x={x}, u={u}, v={v}, q-value, {optimal_q_learning[x,u,v]}\")\n",
    "    print(f\"u,v = {minmin(optimal_q_learning,x)[-1]}\")\n",
    "    print(\"____\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Q LEARNING CONVERGENCE\n",
    "\n",
    "# Plotting\n",
    "for x in range(M + 1):\n",
    "    fig, axs = plt.subplots(1,2, figsize=(14, 5))  # Create a figure with 2 subplots side by side\n",
    "\n",
    "    # Plot with changer[x][u][v][10000:]\n",
    "    for u in range(2):\n",
    "        for v in range(2):\n",
    "            axs[0].plot(range(len(changer[x][u][v][:])), changer[x][u][v][:], label=f'u={u}, v={v}')\n",
    "    axs[0].set_title(f'Plots for x={x} (Entire Data)')\n",
    "    axs[0].set_xlabel('Index')\n",
    "    axs[0].set_ylabel('Value')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Plot with changer[x][u][v][:]\n",
    "    for u in range(2):\n",
    "        for v in range(2):\n",
    "            axs[1].plot(range(len(changer[x][u][v][10000:])), changer[x][u][v][10000:], label=f'u={u}, v={v}')\n",
    "    axs[1].set_title(f'Plots for x={x} (10000:)')\n",
    "    axs[1].set_xlabel('Index')\n",
    "    axs[1].set_ylabel('Value')\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.savefig(f\"Experiment{experiment}_qlearning_subplots{x}.png\", dpi=300)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## DUAL Q LEARNING \n",
    "\n",
    "\n",
    "############ RISK + CLASSICAL Q LEARNING #################\n",
    "\n",
    "delta_store = []\n",
    "delta_store_slow = []\n",
    "time_store = []\n",
    "changer = [[[[[None for _ in range(0)] for _ in range(2)] for _ in range(2)] for _ in range(2)] for _ in range(N + 1) ]\n",
    "\n",
    "\n",
    "changer_slow = [[[[[None for _ in range(0)] for _ in range(2)] for _ in range(2)] for _ in range(2)] for _ in range(N + 1) ]\n",
    "\n",
    "def dual_q_learning(beta=0.9, theta=1e-5):\n",
    "    \"\"\"Perform Q-value iteration to find the optimal action-value function.\"\"\"\n",
    "    # Initialize the Q-values arbitrarily\n",
    "\n",
    "    Q_fast = np.multiply(np.random.rand(N + 1,2,2,2)+1,100)\n",
    "    Q_slow = np.multiply(np.random.rand(N+1,2,2)+1, 100)\n",
    "\n",
    "    # Iterate until convergence\n",
    "    \n",
    "    step=0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        delta_slow = 0\n",
    "\n",
    "        a_n = a_learning_rate(step)\n",
    "        b_n = b_learning_rate(step)\n",
    "\n",
    "        for x in states_x:\n",
    "            for y in states_y:\n",
    "                for u in actions_u:\n",
    "                    for v in actions_v:\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "                        old_q_value = Q_fast[x,y,u,v]\n",
    "                        new_q_value = 0\n",
    "                        arrival = np.random.choice([0, 1], p=[1-p, p])\n",
    "                        x_next = min(N, x - min(u,x) + v*arrival)\n",
    "                        y_next = update_markov_chain_state(y)\n",
    "                        normalization_factor = 1\n",
    "                        cost_term = cost(x)\n",
    "                        if x_next < M+1:\n",
    "                            identity = 1\n",
    "                            miniminQ, _ = minmin2(Q_fast, x_next,y_next)\n",
    "\n",
    "                            change = a_n * (identity * cost_term * miniminQ)/normalization_factor\n",
    "                        else:\n",
    "                            identity=0\n",
    "                            change = 0\n",
    "                        new_q_value = (1-a_n)*old_q_value + a_n*(change)\n",
    "\n",
    "                        changer[x][y][u][v].append(abs(a_n*(change - old_q_value)))\n",
    "                \n",
    "                        Q_fast[x,y,u,v] = new_q_value\n",
    "\n",
    "\n",
    "                        delta = max(delta,old_q_value-new_q_value)\n",
    "######################################################################################\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "                        if step%25==0:\n",
    "\n",
    "                            old_q_value_slow = Q_slow[x,y,u]\n",
    "                            new_q_value_slow = 0\n",
    "                            classic_cost_term = k(i,y,u)\n",
    "\n",
    "                            if identity==1:\n",
    "\n",
    "                                norm_factor = minmin2(Q_fast,0,0)[0]\n",
    "                                numerator = identity * minmin2(Q_fast, x_next, y_next)[0]\n",
    "                                denominator = minmin2(Q_fast,x,y)[0]*norm_factor\n",
    "                                big_term = numerator/denominator\n",
    "\n",
    "                                change_slow = classic_cost_term + beta*big_term*np.min(Q_slow[x_next,y_next,:])\n",
    "                            else:\n",
    "                                change_slow = classic_cost_term \n",
    "\n",
    "                            new_q_value_slow = (1-b_n)*(old_q_value_slow) + b_n*(change_slow)\n",
    "\n",
    "\n",
    "                            changer_slow[x][y][u][v].append(abs(b_n*(change_slow - old_q_value_slow)))\n",
    "\n",
    "                            Q_slow[x,y,u] = new_q_value_slow\n",
    "\n",
    "                            delta_slow = max(delta_slow, old_q_value_slow - new_q_value_slow)\n",
    "######################################################################################\n",
    "\n",
    "                        \n",
    "        if step%25==0:\n",
    "            print(\"step\", step, \"del\",delta, \"del slow\", delta_slow)\n",
    "            delta_store_slow.append(delta_slow)\n",
    "        delta_store.append(delta)\n",
    "        time_store.append(step)\n",
    "\n",
    "    \n",
    "        step+=1\n",
    "        # Check for convergence\n",
    "        \n",
    "        if delta < theta and delta_slow < theta*10:\n",
    "            break\n",
    "\n",
    "\n",
    "    return Q_fast, Q_slow\n",
    "\n",
    "optimal_dual_q_learning  = dual_q_learning()\n",
    "print(optimal_dual_q_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### DUAL Q LEARNING CONVERGENCE\n",
    "\n",
    "\n",
    "# Plotting\n",
    "for x in range(M + 1):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 10))  # Create a figure with 2x2 subplots\n",
    "\n",
    "    # Plot changer[x][u][v][:]\n",
    "    for u in range(2):\n",
    "        for v in range(2):\n",
    "            axs[0, 0].plot(range(len(changer[x][y][u][v][:])), changer[x][y][u][v][:], label=f'u={u}, v={v}')\n",
    "    axs[0, 0].set_title(f'Plots for changer, x={x} (Entire Data)')\n",
    "    axs[0, 0].set_xlabel('Index')\n",
    "    axs[0, 0].set_ylabel('Value')\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    # Plot changer[x][u][v][10000:]\n",
    "    for u in range(2):\n",
    "        for v in range(2):\n",
    "            axs[0, 1].plot(range(10000, len(changer[x][y][u][v][:])), changer[x][y][u][v][10000:], label=f'u={u}, v={v}')\n",
    "    axs[0, 1].set_title(f'Plots for changer, x={x} (10000:)')\n",
    "    axs[0, 1].set_xlabel('Index')\n",
    "    axs[0, 1].set_ylabel('Value')\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    # Plot changer_slow[x][u][v][:]\n",
    "    for u in range(2):\n",
    "        for v in range(2):\n",
    "            axs[1, 0].plot(range(len(changer_slow[x][y][u][v][:])), changer_slow[x][y][u][v][:], label=f'u={u}, v={v}')\n",
    "    axs[1, 0].set_title(f'Plots for changer_slow, x={x} (Entire Data)')\n",
    "    axs[1, 0].set_xlabel('Index')\n",
    "    axs[1, 0].set_ylabel('Value')\n",
    "    axs[1, 0].legend()\n",
    "\n",
    "    # Plot changer_slow[x][u][v][10000:]\n",
    "    for u in range(2):\n",
    "        for v in range(2):\n",
    "            axs[1, 1].plot(range(10000, len(changer_slow[x][y][u][v][:])), changer_slow[x][y][u][v][10000:], label=f'u={u}, v={v}')\n",
    "    axs[1, 1].set_title(f'Plots for changer_slow, x={x} (10000:)')\n",
    "    axs[1, 1].set_xlabel('Index')\n",
    "    axs[1, 1].set_ylabel('Value')\n",
    "    axs[1, 1].legend()\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout automatically to prevent overlap\n",
    "\n",
    "    plt.savefig(f\"Experiment{experiment}_dual_qlearning_subplots{x}.png\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### DUAL Q LEARNING POLICY \n",
    "\n",
    "Q_fast, Q_slow = optimal_dual_q_learning\n",
    "\n",
    "\n",
    "def find_optimal_pairs(Q_slow, Q_fast):\n",
    "   \n",
    "    optimal_pairs = []\n",
    "    \n",
    "    for x in range(N+1):\n",
    "        for y in range(2):\n",
    "            # Find all indices v that minimize Q_fast(x, y, u, v)\n",
    "            v_indices = np.where(Q_fast[x, y].sum(axis=0) == np.min(Q_fast[x, y].sum(axis=0)))[0]\n",
    "            \n",
    "            for v_index in v_indices:\n",
    "                # Find all indices u that minimize Q_slow(x, y, u)\n",
    "                u_indices = np.where(Q_slow[x, y] == np.min(Q_slow[x, y]))[0]\n",
    "                \n",
    "                for u_index in u_indices:\n",
    "                    # Store optimal pair (u_index, v_index)\n",
    "                    optimal_pairs.append((x, y, u_index, v_index))\n",
    "    \n",
    "    return optimal_pairs\n",
    "\n",
    "# Example function call\n",
    "optimal_pairs = find_optimal_pairs(Q_slow, Q_fast)\n",
    "\n",
    "def print_optimal_pairs(optimal_pairs):\n",
    "    # Iterate over unique (x, y) pairs\n",
    "    x_values = set(pair[0] for pair in optimal_pairs)\n",
    "    y_values = set(pair[1] for pair in optimal_pairs)\n",
    "    \n",
    "    for x in x_values:\n",
    "        for y in y_values:\n",
    "            # Filter pairs for current (x, y) pair\n",
    "            pairs_for_xy = [(u, v) for (x_, y_, u, v) in optimal_pairs if x_ == x and y_ == y]\n",
    "            \n",
    "            # Print results for current (x, y) pair\n",
    "            print(f\"Optimal pairs for (x={x}, y={y}):\")\n",
    "            for (u, v) in pairs_for_xy:\n",
    "                print(f\"  u = {u}, v = {v}\")\n",
    "                print(\"___\")\n",
    "\n",
    "\n",
    "print_optimal_pairs(optimal_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### VALUE ITERATION AND Q LEARNING COMPARISON \n",
    "\n",
    "\n",
    "\n",
    "# Normalize Q-values for each queue length separately between 0 and 1\n",
    "def normalize_q_values(optimal_q_values):\n",
    "    optimal_q_values_normalized = np.zeros_like(optimal_q_values)\n",
    "    for x in range(21):\n",
    "        q_values = optimal_q_values[x, :, :].flatten()  # Flatten Q-values for all pairs (u,v) for this queue length\n",
    "        q_min, q_max = q_values.min(), q_values.max()\n",
    "        if q_min != q_max:  # Avoid division by zero\n",
    "            optimal_q_values_normalized[x, :, :] = (optimal_q_values[x, :, :] - q_min) / (q_max - q_min)\n",
    "        else:\n",
    "            optimal_q_values_normalized[x, :, :].fill(0.5)  # Set all normalized Q-values to 0.5 if they are all equal\n",
    "    return optimal_q_values_normalized\n",
    "\n",
    "# Normalized Q-values\n",
    "optimal_q_values_normalized = normalize_q_values(optimal_q_values)\n",
    "optimal_q_learning_normalized = normalize_q_values(optimal_q_learning)\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Plot each pair in the first subplot\n",
    "colors = ['#FF4D4D', '#006400', 'red', '#008080']\n",
    "labels = ['(u=0, v=0)', '(u=0, v=1)', '(u=1, v=0)', '(u=1, v=1)']\n",
    "\n",
    "for i, (u, v) in enumerate([(0, 0), (0, 1), (1, 0), (1, 1)]):\n",
    "    axs[0].scatter(range(19), optimal_q_values_normalized[:19, u, v], color=colors[i], label=labels[i], linewidth=2, marker=\"o\", s=100)\n",
    "\n",
    "axs[0].set_title('Q-values (Normalized) for Different Pairs (Q-Value Iteration)', fontsize=16)\n",
    "axs[0].set_xlabel('Queue length', fontsize=14)\n",
    "axs[0].set_ylabel('Normalized Q-value', fontsize=14)\n",
    "axs[0].set_xticks(range(19))\n",
    "axs[0].legend(loc='upper center')\n",
    "axs[0].grid(False)\n",
    "\n",
    "# Plot each pair in the second subplot\n",
    "for i, (u, v) in enumerate([(0, 0), (0, 1), (1, 0), (1, 1)]):\n",
    "    axs[1].scatter(range(19), optimal_q_learning_normalized[:19, u, v], color=colors[i], label=labels[i], linewidth=2, marker=\"x\", s=100)\n",
    "\n",
    "axs[1].set_title('Q-values (Normalized) for Different Pairs (Q-Learning)', fontsize=16)\n",
    "axs[1].set_xlabel('Queue length', fontsize=14)\n",
    "axs[1].set_ylabel('Normalized Q-value', fontsize=14)\n",
    "axs[1].set_xticks(range(19))\n",
    "axs[1].legend(loc='upper center')\n",
    "axs[1].grid(False)\n",
    "\n",
    "# Save the combined figure\n",
    "plt.savefig(f'Combined_Experiment{experiment}.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## DUAL Q LEARNING - QUEUE TRAJECTORY\n",
    "\n",
    "\n",
    "y_init = 0\n",
    "for i_init in [0,4,6,8,10,14,15,16,17,18]:\n",
    "\n",
    "    i_traj = []\n",
    "    actions_traj = []\n",
    "\n",
    "    i = i_init\n",
    "    y = y_init\n",
    "    time=0\n",
    "    for time in range(60):\n",
    "\n",
    "        (u,v) = [(u, v) for (x_, y_, u, v) in optimal_pairs if x_ == i and y_ == y][0]\n",
    "\n",
    "        i_traj.append(i)\n",
    "        actions_traj.append((u,v))\n",
    "        print(i)\n",
    "        print((u,v))\n",
    "        i = min(N, i - min(i,u) + v*np.random.choice(2))\n",
    "        y = update_markov_chain_state(y)\n",
    "\n",
    "\n",
    "    # Number of time steps\n",
    "    time_steps = range(len(i_traj))\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Plot the queue length over time\n",
    "    ax.plot(time_steps, i_traj, marker='o', linestyle='-', color='b', label='Queue Length')\n",
    "\n",
    "    # Initialize legend handles and labels for the arrows\n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "\n",
    "    # Annotate the actions on the plot\n",
    "    for t, (i, action) in enumerate(zip(time_steps, actions_traj)):\n",
    "        u, v = action\n",
    "        \n",
    "        # Annotate departures when u = 1\n",
    "        if u == 1:\n",
    "            if v == 0:\n",
    "                # Handle simultaneous departure and arrival blocking with different arrow styles\n",
    "                arrow_style = '-|>'\n",
    "                arrow_color = 'red'\n",
    "                label = 'Departure & Blocked Arrival (u=1, v=0)'\n",
    "            else:\n",
    "                arrow_style = '->'\n",
    "                arrow_color = 'green'\n",
    "                label = 'Departure (u=1)'\n",
    "        # Annotate arrival blocking when v = 0\n",
    "        elif v == 0:\n",
    "            arrow_style = '-|>'\n",
    "            arrow_color = 'purple'\n",
    "            label = 'Arrival Blocking (v=0)'\n",
    "        else:\n",
    "            continue  # Skip if neither u nor v is 0\n",
    "        \n",
    "        # Annotate arrows\n",
    "        ax.annotate(\"\", xy=(t, i_traj[t]), xytext=(0, 20), textcoords='offset points', arrowprops=dict(arrowstyle=arrow_style, linewidth=2, color=arrow_color))\n",
    "        \n",
    "        # Append legend handles and labels for the arrows\n",
    "        legend_handles.append(mpatches.Patch(color=arrow_color))\n",
    "        legend_labels.append(label)\n",
    "\n",
    "    # Add legend for the arrows\n",
    "    ax.legend(handles=legend_handles, labels=legend_labels, loc='upper left', fontsize=10)\n",
    "\n",
    "    # Create color legend for the arrow explanations\n",
    "    color_legend_handles = [mpatches.Patch(color='red', label='Departure & Blocked Arrival (u=1, v=0)'),\n",
    "                            mpatches.Patch(color='green', label='Departure (u=1)'),\n",
    "                            mpatches.Patch(color='purple', label='Arrival Blocking (v=0)')]\n",
    "    ax.legend(handles=color_legend_handles, loc='lower right', fontsize=10)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Time', fontsize=12)\n",
    "    ax.set_ylabel('Queue Length', fontsize=12)\n",
    "    ax.set_title(f'Queue Length Over Time, Initial length i = {i_init}', fontsize=14)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Show plot\n",
    "    plt.savefig(f'Experiment{experiment}_queue{i_init}_RISKANDCLASSIClearning.png',dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Q LEARNING / VALUE ITERATION - QUEUE TRAJECTORY \n",
    "\n",
    "\n",
    "\n",
    "for i_init in [0,4,6,8,10,14,15,16,17,18]:\n",
    "\n",
    "    i_traj = []\n",
    "    actions_traj = []\n",
    "\n",
    "    i = i_init\n",
    "    time=0\n",
    "    for time in range(60):\n",
    "\n",
    "        (u,v) = minmin(optimal_q_learning, i)[-1][np.random.choice(len(minmin(optimal_q_learning, i)[-1]))]\n",
    "\n",
    "        i_traj.append(i)\n",
    "        actions_traj.append((u,v))\n",
    "        print(i)\n",
    "        print((u,v))\n",
    "        i = min(N, i - min(i,u) + v*np.random.choice(2))\n",
    "\n",
    "\n",
    "    # Number of time steps\n",
    "    time_steps = range(len(i_traj))\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Plot the queue length over time\n",
    "    ax.plot(time_steps, i_traj, marker='o', linestyle='-', color='b', label='Queue Length')\n",
    "\n",
    "    # Initialize legend handles and labels for the arrows\n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "\n",
    "    # Annotate the actions on the plot\n",
    "    for t, (i, action) in enumerate(zip(time_steps, actions_traj)):\n",
    "        u, v = action\n",
    "        \n",
    "        # Annotate departures when u = 1\n",
    "        if u == 1:\n",
    "            if v == 0:\n",
    "                # Handle simultaneous departure and arrival blocking with different arrow styles\n",
    "                arrow_style = '-|>'\n",
    "                arrow_color = 'red'\n",
    "                label = 'Departure & Blocked Arrival (u=1, v=0)'\n",
    "            else:\n",
    "                arrow_style = '->'\n",
    "                arrow_color = 'green'\n",
    "                label = 'Departure (u=1)'\n",
    "        # Annotate arrival blocking when v = 0\n",
    "        elif v == 0:\n",
    "            arrow_style = '-|>'\n",
    "            arrow_color = 'purple'\n",
    "            label = 'Arrival Blocking (v=0)'\n",
    "        else:\n",
    "            continue  # Skip if neither u nor v is 0\n",
    "        \n",
    "        # Annotate arrows\n",
    "        ax.annotate(\"\", xy=(t, i_traj[t]), xytext=(0, 20), textcoords='offset points', arrowprops=dict(arrowstyle=arrow_style, linewidth=2, color=arrow_color))\n",
    "        \n",
    "        # Append legend handles and labels for the arrows\n",
    "        legend_handles.append(mpatches.Patch(color=arrow_color))\n",
    "        legend_labels.append(label)\n",
    "\n",
    "    # Add legend for the arrows\n",
    "    ax.legend(handles=legend_handles, labels=legend_labels, loc='upper left', fontsize=10)\n",
    "\n",
    "    # Create color legend for the arrow explanations\n",
    "    color_legend_handles = [mpatches.Patch(color='red', label='Departure & Blocked Arrival (u=1, v=0)'),\n",
    "                            mpatches.Patch(color='green', label='Departure (u=1)'),\n",
    "                            mpatches.Patch(color='purple', label='Arrival Blocking (v=0)')]\n",
    "    ax.legend(handles=color_legend_handles, loc='lower right', fontsize=10)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Time', fontsize=12)\n",
    "    ax.set_ylabel('Queue Length', fontsize=12)\n",
    "    ax.set_title(f'Queue Length Over Time, Initial length i = {i_init}', fontsize=14)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Show plot\n",
    "    plt.savefig(f'Experiment{experiment}_queue{i_init}_learning.png',dpi=300)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
